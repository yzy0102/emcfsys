{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d31780",
   "metadata": {},
   "source": [
    "# Test SR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb49aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ÂÅáËÆæ‰Ω†ÁöÑÈ°πÁõÆË∑ØÂæÑÈÖçÁΩÆÊ≠£Á°Æ\n",
    "from src.emcfsys.EMCellFiner.hat.models.hat_model import HATModel\n",
    "from src.emcfsys.EMCellFiner.hat.models.img_utils import tensor2img\n",
    "\n",
    "# 1. ÂàùÂßãÂåñÊ®°Âûã\n",
    "path = r\"D:\\napari_EMCF\\EMCFsys\\models\\EMCellFiner.pth\"\n",
    "# ÊòæÂºèÊåáÂÆö tile_sizeÔºåÈò≤Ê≠¢ÊòæÂ≠òÊ∫¢Âá∫ÔºõÂØπ‰∫éÂ∞èÂõæÂèØ‰ª•‰∏çÁî® tile\n",
    "model = HATModel(scale=4, tile_size=512) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# 2. ËØªÂèñ‰∏éÈ¢ÑÂ§ÑÁêÜ\n",
    "img_path = r\"D:\\napari_EMCF\\EMCFsys\\emcfsys\\image\\Bock2011_2951_XrV1ciGgTWHjepNf.tif\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# ËΩ¨Êç¢‰∏∫ Numpy Âπ∂ÂΩí‰∏ÄÂåñ\n",
    "img_np = np.array(img).astype(np.float32) / 255.\n",
    "# ËΩ¨Êç¢‰∏∫ Tensor: (H, W, C) -> (C, H, W) -> (1, C, H, W)\n",
    "img_torch = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "print(f\"Input Shape: {img_torch.shape}\")\n",
    "# 3. Êé®ÁêÜ\n",
    "with torch.no_grad():\n",
    "    output = model(img_torch) \n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "# 4. ÂêéÂ§ÑÁêÜ\n",
    "output = output.cpu()\n",
    "img_out = tensor2img(output, rgb2bgr=False, min_max=(0, 1))\n",
    "# 5. ËΩ¨Âõû PIL ÂõæÁâá\n",
    "img_final = Image.fromarray(img_out)\n",
    "\n",
    "# È™åËØÅÁªìÊûú\n",
    "# img_final.show() \n",
    "# img_final.save(\"result_sr.png\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7665c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "c:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the model from torch hub : https://github.com/yzy0102/emcfsys/releases/latest/download/EMCellFiner.pth\n",
      "üïí ÂáΩÊï∞ 'prepare_image' ÊâßË°åÂÆåÊàêÔºåËÄóÊó∂: 0.0035 Áßí„ÄÇ\n",
      "\tTile 1/1\n",
      "üïí ÂáΩÊï∞ 'hat_infer_numpy' ÊâßË°åÂÆåÊàêÔºåËÄóÊó∂: 4.2623 Áßí„ÄÇ\n",
      "inference time:  4.740279674530029\n"
     ]
    }
   ],
   "source": [
    "from src.emcfsys.EMCellFiner.hat.models.inference_hat import hat_infer_numpy\n",
    "from src.emcfsys.EMCellFiner.hat.models.hat_model import HATModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "start = time.time()\n",
    "img_path = r\"D:\\napari_EMCF\\EMCFsys\\emcfsys\\image\\Bock2011_2951_XrV1ciGgTWHjepNf.tif\"\n",
    "img = Image.open(img_path).convert(\"RGB\").crop([0,0,512,512])\n",
    "img_np = np.array(img)\n",
    "\n",
    "model = HATModel(scale=4, tile_size=512)\n",
    "device = \"cuda\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "out = hat_infer_numpy(\n",
    "    model= model,\n",
    "    image= img_np,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(\"inference time: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c826e5",
   "metadata": {},
   "source": [
    "# test SR stack images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9cb8c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "c:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the model from torch hub : https://github.com/yzy0102/emcfsys/releases/latest/download/EMCellFiner.pth\n",
      "üïí ÂáΩÊï∞ 'prepare_image' ÊâßË°åÂÆåÊàêÔºåËÄóÊó∂: 0.0020 Áßí„ÄÇ\n",
      "\tTile 1/4\n",
      "\tTile 2/4\n",
      "\tTile 3/4\n",
      "\tTile 4/4\n",
      "model inference used: 16.723570108413696\n",
      "tensor2img used: 0.13810420036315918\n",
      "üïí ÂáΩÊï∞ 'hat_infer_numpy' ÊâßË°åÂÆåÊàêÔºåËÄóÊó∂: 17.0184 Áßí„ÄÇ\n",
      "inference time:  17.362804174423218\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from src.emcfsys.EMCellFiner.hat.models.inference_hat import hat_infer_numpy\n",
    "from src.emcfsys.EMCellFiner.hat.models.hat_model import HATModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "start = time.time()\n",
    "# img_path = r\"D:\\napari_EMCF\\EMCFsys\\emcfsys\\image\\Bock2011_2951_XrV1ciGgTWHjepNf.tif\"\n",
    "# img = Image.open(img_path).convert(\"RGB\").crop([0,0,512,512])\n",
    "\n",
    "stack_path = [r\"D:\\CellChange\\Bock3D\\image\\3004.jpg\",\n",
    "              r\"D:\\CellChange\\Bock3D\\image\\3044.jpg\",\n",
    "              r\"D:\\CellChange\\Bock3D\\image\\3054.jpg\",]\n",
    "\n",
    "stack_path = [r\"D:\\CellChange\\Bock3D\\image\\3004.jpg\",]\n",
    "\n",
    "stack_imgs = np.array([np.array(Image.open(p).convert(\"L\").resize([1024,1024])) for p in stack_path])\n",
    "\n",
    "\n",
    "model = HATModel(scale=4, tile_size=512)\n",
    "device = \"cuda\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "out = hat_infer_numpy(\n",
    "    model= model,\n",
    "    image= stack_imgs,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(\"inference time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c885825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "img = Image.open(urlopen(\n",
    "    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "))\n",
    "\n",
    "model = timm.create_model('vit_small_patch16_dinov3.lvd1689m', pretrained=True)\n",
    "model = model.eval()\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n",
    "\n",
    "top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21c549",
   "metadata": {},
   "source": [
    "# test the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c789f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "c:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\albumentations\\check_version.py:147: UserWarning: Error fetching version info The read operation timed out\n",
      "  data = fetch_version_info()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights loaded successfully.\n",
      "[EMCellFound] Loading pretrained weights from: https://github.com/yzy0102/emcfsys/releases/latest/download/MAE_EMCellFoundVit_base_224_inEMCF.pth\n",
      "Load pretrained weights from: {pretrained_url}\n",
      "Pretrained weights loaded successfully.\n",
      "New best model found at epoch 1! IoU=0.4354\n",
      "New best model found at epoch 2! IoU=0.4615\n",
      "New best model found at epoch 10! IoU=0.4683\n",
      "New best model found at epoch 11! IoU=0.4737\n",
      "New best model found at epoch 12! IoU=0.4910\n"
     ]
    }
   ],
   "source": [
    "from src.emcfsys.EMCellFound.train import train_loop\n",
    "\n",
    "img_dir = r\"D:\\napari_EMCF\\EMCFsys\\emcfsys\\image\"\n",
    "label_dir = r\"D:\\napari_EMCF\\EMCFsys\\emcfsys\\label\"\n",
    "save_dir = r\"D:\\napari_EMCF\\EMCFsys\\models\"\n",
    "\n",
    "train_loop(img_dir, \n",
    "           label_dir, \n",
    "           save_dir, \n",
    "           model_name='pspnet',\n",
    "           backbone_name='emcellfound_vit_base',\n",
    "           pretrained = True,\n",
    "           pretrained_model=None,\n",
    "           lr=1e-3, batch_size=4, \n",
    "           epochs=100, device=None,\n",
    "           callback=None, target_size=(256, 256), \n",
    "           classes_num=2, ignore_index=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e6c9c",
   "metadata": {},
   "source": [
    "# Inference the seg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c966b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights loaded successfully.\n",
      "Pretrained weights loaded successfully.\n",
      "Warning: load_state_dict failed, trying strict=False\n",
      "üïí ÂáΩÊï∞ 'prepare_image' ÊâßË°åÂÆåÊàêÔºåËÄóÊó∂: 0.0102 Áßí„ÄÇ\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Input width (1024) doesn't match model (512).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m img = np.array(Image.open(img_path).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m).resize((\u001b[32m1024\u001b[39m, \u001b[32m512\u001b[39m)))\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Êé®ÁêÜ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m mask = \u001b[43minfer_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(mask.shape, np.unique(mask))\n\u001b[32m     28\u001b[39m Image.fromarray(mask*\u001b[32m255\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\napari_EMCF\\EMCFsys\\emcfsys\\src\\emcfsys\\EMCellFound\\inference.py:33\u001b[39m, in \u001b[36mtimer.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     30\u001b[39m start_time = time.time()\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# ÊâßË°åÂéüÂáΩÊï∞Âπ∂Ëé∑ÂèñÁªìÊûú\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# ËÆ∞ÂΩïÁªìÊùüÊó∂Èó¥\u001b[39;00m\n\u001b[32m     36\u001b[39m end_time = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\napari_EMCF\\EMCFsys\\emcfsys\\src\\emcfsys\\EMCellFound\\inference.py:212\u001b[39m, in \u001b[36minfer_numpy\u001b[39m\u001b[34m(model, image, device)\u001b[39m\n\u001b[32m    209\u001b[39m x = prepare_image(image, in_channels=\u001b[32m3\u001b[39m).to(device)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     out, _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# 1,C,H,W\u001b[39;00m\n\u001b[32m    213\u001b[39m     \u001b[38;5;66;03m# print(out.shape)\u001b[39;00m\n\u001b[32m    214\u001b[39m     mask = torch.argmax(out, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\napari_EMCF\\EMCFsys\\emcfsys\\src\\emcfsys\\EMCellFound\\models\\DeepLabv3Plus.py:114\u001b[39m, in \u001b[36mDeepLabV3Plus.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# features: [C1, C2, C3, C4, C5]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     feats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# print(\"feats:\", [f.shape for f in feats])\u001b[39;00m\n\u001b[32m    116\u001b[39m     low = feats[\u001b[32m1\u001b[39m]      \u001b[38;5;66;03m# C2\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\napari_EMCF\\EMCFsys\\emcfsys\\src\\emcfsys\\EMCellFound\\models\\BackboneWrapper.py:197\u001b[39m, in \u001b[36mCasualBackbones.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     feats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;66;03m# print(\"feats in casual backbones:\", [f.shape for f in feats])\u001b[39;00m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.using_adapter:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\napari_EMCF\\EMCFsys\\emcfsys\\src\\emcfsys\\EMCellFound\\models\\EMCellFoundViT.py:164\u001b[39m, in \u001b[36mEMCellFoundViT.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# rerturn features from specified stages\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_intermediate_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# unpatch\u001b[39;00m\n\u001b[32m    166\u001b[39m features = [\u001b[38;5;28mself\u001b[39m.to_feature_map(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\timm\\models\\vision_transformer.py:924\u001b[39m, in \u001b[36mVisionTransformer.get_intermediate_layers\u001b[39m\u001b[34m(self, x, n, reshape, return_prefix_tokens, norm, attn_mask)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_intermediate_layers\u001b[39m(\n\u001b[32m    902\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    903\u001b[39m         x: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    908\u001b[39m         attn_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    909\u001b[39m ) -> List[torch.Tensor]:\n\u001b[32m    910\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get intermediate layer outputs (DINO interface compatibility).\u001b[39;00m\n\u001b[32m    911\u001b[39m \n\u001b[32m    912\u001b[39m \u001b[33;03m    NOTE: This API is for backwards compat, favour using forward_intermediates() directly.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m \u001b[33;03m        List of intermediate features.\u001b[39;00m\n\u001b[32m    923\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_intermediates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_prefix_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_prefix_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_fmt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNCHW\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNLC\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediates_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\timm\\models\\vision_transformer.py:816\u001b[39m, in \u001b[36mVisionTransformer.forward_intermediates\u001b[39m\u001b[34m(self, x, indices, return_prefix_tokens, norm, stop_early, output_fmt, intermediates_only, output_dict, attn_mask)\u001b[39m\n\u001b[32m    814\u001b[39m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[32m    815\u001b[39m B, _, height, width = x.shape\n\u001b[32m--> \u001b[39m\u001b[32m816\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    817\u001b[39m x = \u001b[38;5;28mself\u001b[39m._pos_embed(x)\n\u001b[32m    818\u001b[39m x = \u001b[38;5;28mself\u001b[39m.patch_drop(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\timm\\layers\\patch_embed.py:122\u001b[39m, in \u001b[36mPatchEmbed.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strict_img_size:\n\u001b[32m    121\u001b[39m     _assert(H == \u001b[38;5;28mself\u001b[39m.img_size[\u001b[32m0\u001b[39m], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput height (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.img_size[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mInput width (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mW\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m) doesn\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mt match model (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m).\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dynamic_img_pad:\n\u001b[32m    124\u001b[39m     _assert(\n\u001b[32m    125\u001b[39m         H % \u001b[38;5;28mself\u001b[39m.patch_size[\u001b[32m0\u001b[39m] == \u001b[32m0\u001b[39m,\n\u001b[32m    126\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput height (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) should be divisible by patch size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.patch_size[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    127\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\torch\\__init__.py:2040\u001b[39m, in \u001b[36m_assert\u001b[39m\u001b[34m(condition, message)\u001b[39m\n\u001b[32m   2034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.Tensor \u001b[38;5;129;01mand\u001b[39;00m overrides.has_torch_function(\n\u001b[32m   2035\u001b[39m     (condition,)\n\u001b[32m   2036\u001b[39m ):\n\u001b[32m   2037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m overrides.handle_torch_function(\n\u001b[32m   2038\u001b[39m         _assert, (condition,), condition, message\n\u001b[32m   2039\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2040\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
      "\u001b[31mAssertionError\u001b[39m: Input width (1024) doesn't match model (512)."
     ]
    }
   ],
   "source": [
    "from src.emcfsys.EMCellFound.inference import infer_numpy, load_model\n",
    "from src.emcfsys.EMCellFound.utils.checkpoint import load_pretrained\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from src.emcfsys.EMCellFound.models.model_factory import get_model\n",
    "from src.emcfsys.EMCellFound.models.DeepLabv3Plus import DeepLabV3Plus\n",
    "\n",
    "# img_path = r\"D:\\napari_EMCF\\EMCFsys\\emcfsys\\image\\Bock2011_2951_xZGgUFHBDISQMdco.tif\"\n",
    "img_path = r\"D:\\CellChange\\Bock3D\\image\\3004.jpg\"\n",
    "\n",
    "\n",
    "model_path = r\"D:\\napari_EMCF\\EMCFsys\\models\\best_model_epoch98_IoU=0.9024.pth\"\n",
    "# ÊûÑÂª∫ + Âä†ËΩΩÊ®°Âûã\n",
    "model = load_model(model_name=\"deeplabv3plus\", \n",
    "                   backbone_name=\"emcellfound_vit_base\", \n",
    "                   num_classes=2, \n",
    "                   model_path=model_path, \n",
    "                   aux_on=False, \n",
    "                   device=\"cuda\")\n",
    "\n",
    "# Âä†ËΩΩÂõæÂÉè\n",
    "img = np.array(Image.open(img_path).convert(\"RGB\").resize((512, 512)))\n",
    "\n",
    "# Êé®ÁêÜ\n",
    "mask = infer_numpy(model, img, device=\"cuda\")\n",
    "print(mask.shape, np.unique(mask))\n",
    "Image.fromarray(mask*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db57e73",
   "metadata": {},
   "source": [
    "# test the infer_numpy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d06d476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2048, 2048, 3)\n",
      "Pretrained weights loaded successfully.\n",
      "Pretrained weights loaded successfully.\n",
      "Warning: load_state_dict failed, trying strict=False\n",
      "üïí ÂáΩÊï∞ 'prepare_image' ÊâßË°åÂÆåÊàêÔºåËÄóÊó∂: 0.3536 Áßí„ÄÇ\n",
      "üïí ÂáΩÊï∞ 'infer_full_image' ÊâßË°åÂÆåÊàêÔºåËÄóÊó∂: 0.5843 Áßí„ÄÇ\n",
      "(6, 2048, 2048) [0 1]\n"
     ]
    }
   ],
   "source": [
    "from src.emcfsys.EMCellFound.inference import infer_numpy, load_model, infer_full_image\n",
    "from src.emcfsys.EMCellFound.utils.checkpoint import load_pretrained\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from src.emcfsys.EMCellFound.models.model_factory import get_model\n",
    "from src.emcfsys.EMCellFound.models.DeepLabv3Plus import DeepLabV3Plus\n",
    "\n",
    "\n",
    "img_path = r\"D:\\napari_EMCF\\EMCFsys\\emcfsys\\image\\Bock2011_2951_xZGgUFHBDISQMdco.tif\"\n",
    "\n",
    "stack_path = [r\"D:\\CellChange\\Bock3D\\image\\3004.jpg\",\n",
    "              r\"D:\\CellChange\\Bock3D\\image\\3014.jpg\",\n",
    "              r\"D:\\CellChange\\Bock3D\\image\\3024.jpg\",\n",
    "              r\"D:\\CellChange\\Bock3D\\image\\3034.jpg\",\n",
    "              r\"D:\\CellChange\\Bock3D\\image\\3044.jpg\",\n",
    "              r\"D:\\CellChange\\Bock3D\\image\\3054.jpg\",]\n",
    "\n",
    "# stack_path = [r\"D:\\CellChange\\Bock3D\\image\\3004.jpg\",]\n",
    "\n",
    "stack_imgs = np.array([np.array(Image.open(p).convert(\"RGB\")) for p in stack_path])\n",
    "print(stack_imgs.shape)\n",
    "\n",
    "\n",
    "model_path = r\"D:\\napari_EMCF\\EMCFsys\\models\\best_model_epoch98_IoU=0.9024.pth\"\n",
    "\n",
    "# ÊûÑÂª∫ + Âä†ËΩΩÊ®°Âûã\n",
    "\n",
    "model = load_model(model_name=\"deeplabv3plus\", \n",
    "                   backbone_name=\"emcellfound_vit_base\", \n",
    "                   num_classes=2, \n",
    "                   model_path=model_path, \n",
    "                   aux_on=False, \n",
    "                   device=\"cuda\")\n",
    "\n",
    "# Âä†ËΩΩÂõæÂÉè\n",
    "\n",
    "# Êé®ÁêÜ\n",
    "mask = infer_full_image(model, stack_imgs, (512,512), device=\"cuda\")\n",
    "print(mask.shape, np.unique(mask))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47957a4",
   "metadata": {},
   "source": [
    "# Test slide inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d40b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2048, 2048, 3)\n",
      "Pretrained weights loaded successfully.\n",
      "Pretrained weights loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\napari_EMCF\\EMCFsys\\emcfsys\\src\\emcfsys\\EMCellFound\\inference.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: load_state_dict failed, trying strict=False\n",
      "üïí ÂáΩÊï∞ 'prepare_image' ÊâßË°åÂÆåÊàêÔºåËÄóÊó∂: 0.3164 Áßí„ÄÇ\n",
      "üïí ÂáΩÊï∞ 'infer_sliding_window' ÊâßË°åÂÆåÊàêÔºåËÄóÊó∂: 4.7105 Áßí„ÄÇ\n",
      "(6, 2048, 2048) [0 1]\n"
     ]
    }
   ],
   "source": [
    "from src.emcfsys.EMCellFound.inference import infer_numpy, load_model, infer_full_image, infer_sliding_window\n",
    "from src.emcfsys.EMCellFound.utils.checkpoint import load_pretrained\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from src.emcfsys.EMCellFound.models.model_factory import get_model\n",
    "from src.emcfsys.EMCellFound.models.DeepLabv3Plus import DeepLabV3Plus\n",
    "\n",
    "\n",
    "img_path = r\"D:\\napari_EMCF\\EMCFsys\\emcfsys\\image\\Bock2011_2951_xZGgUFHBDISQMdco.tif\"\n",
    "\n",
    "stack_path = [r\"D:\\CellChange\\Bock3D\\image\\3004.jpg\",\n",
    "              r\"D:\\CellChange\\Bock3D\\image\\3014.jpg\",\n",
    "              r\"D:\\CellChange\\Bock3D\\image\\3024.jpg\",\n",
    "              r\"D:\\CellChange\\Bock3D\\image\\3034.jpg\",\n",
    "              r\"D:\\CellChange\\Bock3D\\image\\3044.jpg\",\n",
    "              r\"D:\\CellChange\\Bock3D\\image\\3054.jpg\",]\n",
    "\n",
    "# stack_path = [r\"D:\\CellChange\\Bock3D\\image\\3004.jpg\",]\n",
    "\n",
    "stack_imgs = np.array([np.array(Image.open(p).convert(\"RGB\")) for p in stack_path])\n",
    "print(stack_imgs.shape)\n",
    "\n",
    "\n",
    "model_path = r\"D:\\napari_EMCF\\EMCFsys\\models\\best_model_epoch98_IoU=0.9024.pth\"\n",
    "\n",
    "# ÊûÑÂª∫ + Âä†ËΩΩÊ®°Âûã\n",
    "\n",
    "model = load_model(model_name=\"deeplabv3plus\", \n",
    "                   backbone_name=\"emcellfound_vit_base\", \n",
    "                   num_classes=2, \n",
    "                   model_path=model_path, \n",
    "                   aux_on=False, \n",
    "                   device=\"cuda\")\n",
    "\n",
    "# Âä†ËΩΩÂõæÂÉè\n",
    "\n",
    "# Êé®ÁêÜ\n",
    "\n",
    "mask = infer_sliding_window(model=model, image=stack_imgs, window_size= 512, \n",
    "    overlap = 0.25, out_channels=2, img_size = (512,512), device = \"cuda\",)\n",
    "\n",
    "print(mask.shape, np.unique(mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bba3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMCF_napari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
