{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d31780",
   "metadata": {},
   "source": [
    "# Test SR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb49aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设你的项目路径配置正确\n",
    "from src.emcfsys.EMCellFiner.hat.models.hat_model import HATModel\n",
    "from src.emcfsys.EMCellFiner.hat.models.img_utils import tensor2img\n",
    "\n",
    "# 1. 初始化模型\n",
    "path = r\"D:\\napari_EMCF\\EMCFsys\\models\\EMCellFiner.pth\"\n",
    "# 显式指定 tile_size，防止显存溢出；对于小图可以不用 tile\n",
    "model = HATModel(scale=4, tile_size=512) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# 2. 读取与预处理\n",
    "img_path = r\"D:\\napari_EMCF\\EMCFsys\\emcfsys\\image\\Bock2011_2951_XrV1ciGgTWHjepNf.tif\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# 转换为 Numpy 并归一化\n",
    "img_np = np.array(img).astype(np.float32) / 255.\n",
    "# 转换为 Tensor: (H, W, C) -> (C, H, W) -> (1, C, H, W)\n",
    "img_torch = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "print(f\"Input Shape: {img_torch.shape}\")\n",
    "# 3. 推理\n",
    "with torch.no_grad():\n",
    "    output = model(img_torch) \n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "# 4. 后处理\n",
    "output = output.cpu()\n",
    "img_out = tensor2img(output, rgb2bgr=False, min_max=(0, 1))\n",
    "# 5. 转回 PIL 图片\n",
    "img_final = Image.fromarray(img_out)\n",
    "\n",
    "# 验证结果\n",
    "# img_final.show() \n",
    "# img_final.save(\"result_sr.png\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7665c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.emcfsys.EMCellFiner.hat.models.inference_hat import hat_infer_numpy\n",
    "from src.emcfsys.EMCellFiner.hat.models.hat_model import HATModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "start = time.time()\n",
    "img_path = r\"D:\\napari_EMCF\\EMCFsys\\emcfsys\\image\\Bock2011_2951_XrV1ciGgTWHjepNf.tif\"\n",
    "img = Image.open(img_path).convert(\"RGB\").crop([0,0,512,512])\n",
    "img_np = np.array(img)\n",
    "\n",
    "model = HATModel(scale=4, tile_size=512)\n",
    "device = \"cpu\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "out = hat_infer_numpy(\n",
    "    model= model,\n",
    "    image= img_np,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(\"inference time: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70a0d5",
   "metadata": {},
   "source": [
    "# add torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d95b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YZY\\miniconda3\\envs\\EMCF_napari\\Lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the model from torch hub : https://github.com/yzy0102/emcfsys/releases/latest/download/EMCellFiner.pth\n",
      "\tTile 1/1\n",
      "inference time:  4.851733922958374\n"
     ]
    }
   ],
   "source": [
    "from src.emcfsys.EMCellFiner.hat.models.inference_hat import hat_infer_numpy\n",
    "from src.emcfsys.EMCellFiner.hat.models.hat_model import HATModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "start = time.time()\n",
    "img_path = r\"D:\\napari_EMCF\\EMCFsys\\emcfsys\\image\\Bock2011_2951_XrV1ciGgTWHjepNf.tif\"\n",
    "img = Image.open(img_path).convert(\"RGB\").crop([0,0,512,512])\n",
    "img_np = np.array(img)\n",
    "\n",
    "model = HATModel(scale=4, tile_size=512)\n",
    "device = \"cuda\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "out = hat_infer_numpy(\n",
    "    model= model,\n",
    "    image= img_np,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(\"inference time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c826e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16eeb1e0",
   "metadata": {},
   "source": [
    "# Export model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3253e919",
   "metadata": {},
   "source": [
    "## ONNX can't speed up many times1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45697e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from src.emcfsys.EMCellFiner.hat.models.hat_arch import HAT\n",
    "\n",
    "model = HAT()\n",
    "\n",
    "state_dict = torch.load(r\"D:\\napari_EMCF\\EMCFsys\\models\\EMCellFiner.pth\", map_location='cpu')\n",
    "model.load_state_dict(state_dict['params'], strict=True)\n",
    "\n",
    "\n",
    "def export_hat_to_onnx(model, onnx_path=\"HATModel.onnx\", device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    导出 HAT 网络到 ONNX\n",
    "    注意：tile_process 不会导出，只能做整张图推理\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # 示例输入\n",
    "    dummy_input = torch.randn(1, 3, 64, 64, device=device)  # B,C,H,W\n",
    "\n",
    "    # 导出 ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=17,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {2: 'height', 3: 'width'},\n",
    "            'output': {2: 'height', 3: 'width'},\n",
    "        }\n",
    "    )\n",
    "    print(f\"✅ ONNX model saved at {onnx_path}\")\n",
    "\n",
    "export_hat_to_onnx(model, onnx_path=\"HATModel.onnx\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce54b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from src.emcfsys.EMCellFiner.hat.models.img_utils import tensor2img\n",
    "import torch\n",
    "import time\n",
    "start = time.time()\n",
    "ort_sess = ort.InferenceSession(\"HATModel.onnx\")\n",
    "\n",
    "input_name = ort_sess.get_inputs()[0].name\n",
    "output_name = ort_sess.get_outputs()[0].name\n",
    "\n",
    "img_path = r\"src\\emcfsys\\test_imgs\\test_img.tif\"\n",
    "image = np.array(Image.open(img_path).convert(\"RGB\").crop([0,0,512,512]))/255.\n",
    "\n",
    "img_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).numpy().astype(np.float32)\n",
    "\n",
    "out = ort_sess.run([output_name], {input_name: img_tensor})\n",
    "# out[0] to image 1, 3, 256, 256 -> 256, 256, 3\n",
    "out = torch.from_numpy(np.array(out))\n",
    "out = tensor2img(out, rgb2bgr=False, min_max=(0, 1))\n",
    "end = time.time()\n",
    "\n",
    "print(\"inference time: \", end - start)\n",
    "print(out.shape)\n",
    "print(out.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e06b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c885825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMCF_napari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
