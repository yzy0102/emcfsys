# inference.py
import numpy as np
import torch

from .utils.checkpoint import load_model, load_pretrained
from skimage.transform import resize
import torch.nn.functional as F
import numpy as np
# import Optional
from typing import Optional
from .models.model_factory import get_model
import numpy as np
import torch
from skimage.transform import resize
from typing import Optional, Tuple, List, Union

from PIL import Image
import time
import functools

def timer(func):
    """
    ä¸€ä¸ªä¿®é¥°å‡½æ•°ï¼Œç”¨äºè®¡ç®—è¢«è£…é¥°å‡½æ•°çš„æ‰§è¡Œæ—¶é—´å¹¶æ‰“å°ç»“æœã€‚
    """
    # ä½¿ç”¨ functools.wraps ä¿æŒåŸå‡½æ•°çš„åç§°ã€æ–‡æ¡£å­—ç¬¦ä¸²ç­‰å…ƒæ•°æ®
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æ‰§è¡ŒåŸå‡½æ•°å¹¶è·å–ç»“æœ
        result = func(*args, **kwargs)
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        
        # è®¡ç®—å¹¶æ‰“å°æŒç»­æ—¶é—´
        duration = end_time - start_time
        print(f"ğŸ•’ å‡½æ•° '{func.__name__}' æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {duration:.4f} ç§’ã€‚")
        
        # è¿”å›åŸå‡½æ•°çš„æ‰§è¡Œç»“æœ
        return result
        
    return wrapper

# æç¤ºï¼šå¦‚æœéœ€è¦æ›´ç²¾ç¡®æˆ–æ›´ä¾§é‡äº CPU æ—¶é—´çš„è®¡æ—¶ï¼Œ
# å¯ä»¥å°† time.time() æ›¿æ¢ä¸º time.perf_counter()ã€‚


class Normalize:
    """Normalize image to mean/std (mmseg style)."""
    def __init__(self, mean=(123.675, 116.28, 103.53), std=(58.395, 57.12, 57.375)):
        self.mean = np.array(mean, dtype=np.float32)
        self.std = np.array(std, dtype=np.float32)

    def __call__(self, img: np.ndarray):
        # å…¼å®¹ N x C x H x W æ ¼å¼çš„ Batch è¾“å…¥
        if img.ndim == 4:
            # è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å‡è®¾å½’ä¸€åŒ–æ“ä½œå°†åœ¨ prepare_image å¤–éƒ¨çš„ NumPy å¹¿æ’­ä¸­å®Œæˆï¼Œ
            # è¿™é‡Œä¿æŒä¸å˜ï¼Œä»¥é˜² prepare_image å†…éƒ¨é€»è¾‘è°ƒç”¨å®ƒã€‚
            return img
        
        img = img.astype(np.float32)
        # HWC -> CHW if necessary
        if img.ndim == 2:
            img = img[np.newaxis, ...]  # 1,H,W
        elif img.ndim == 3 and img.shape[-1] in (1,3):
            img = np.transpose(img, (2,0,1))
        elif img.ndim == 3 and img.shape[0] in (1,3):
            pass
        else:
            raise ValueError(f"Unsupported image shape: {img.shape}")
        # normalize
        for c in range(img.shape[0]):
            img[c] = (img[c] - self.mean[c]) / self.std[c]
        return img





def load_model(model_name: str, backbone_name: str, num_classes: int, model_path: str, aux_on=True, device=None):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡ï¼ˆstate_dictï¼‰åˆ°æŒ‡å®šæ¨¡å‹
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æ„å»ºæ¨¡å‹
    model = get_model(model_name, backbone_name, num_classes=num_classes, aux_on=aux_on, pretrained=False)
    
    # åŠ è½½æƒé‡
    state_dict = torch.load(model_path, map_location=device)
    
    # æ”¯æŒä¸¤ç§ä¿å­˜æ–¹å¼ï¼šæ•´ä¸ª state_dict æˆ–è€…ç›´æ¥ model å¯¹è±¡
    if isinstance(state_dict, dict) and "state_dict" in state_dict:
        state_dict = state_dict["state_dict"]
    
    try:
        model.load_state_dict(state_dict)
    except RuntimeError as e:
        print("Warning: load_state_dict failed, trying strict=False")
        model.load_state_dict(state_dict, strict=False)
    
    model.to(device)
    model.eval()
    
    return model


@timer
def prepare_image(img: np.ndarray, 
                  in_channels=3, 
                  normalize=True, 
                  mean=(123.675,116.28,103.53), 
                  std=(58.395,57.12,57.375)):
    """
    Prepare image(s) for model inference, unifying the output shape to (N, 3, H, W).
    
    - img: np.ndarray, å¯ä»¥æ˜¯å•å¼ å›¾åƒ (ndim<=3) æˆ–å›¾åƒ Stack (ndim>=4)ã€‚
    - in_channels: æ­¤å‚æ•°å°†è¢«å¿½ç•¥ï¼Œè¾“å‡ºé€šé“æ•°å›ºå®šä¸º 3 (RGB)ã€‚
    
    Returns: torch.Tensor of shape (N, 3, H, W).
    """
    
    # 0. è®¾ç½®ç›®æ ‡é€šé“æ•°
    TARGET_CHANNELS = 3
    arr = np.array(img).astype(np.float32)
    # 1. ç±»å‹è½¬æ¢ä¸º float32
    # arr = img
    
    # 2. ç¡®å®šç»´åº¦å¹¶è½¬æ¢ä¸º N x C_in x H x W æ ¼å¼
    
    # inputx: B, H, W, C  or  H, W, C  or H, W
    if arr.ndim == 2:
        # è¾“å…¥å›¾åƒæ˜¯å•å¼ ç°åº¦å›¾ Hï¼Œ W
        # è½¬åŒ–æˆ (1, 1, h, w)
        arr = np.expand_dims(arr, axis=(0, 1))# 1, 1, H, W

    elif arr.ndim == 3:
        if arr.shape[-1] > 5:
            # case1: è¾“å…¥å›¾åƒå¯èƒ½æ˜¯ B,H,W ä¸€ä¸ªbatchçš„ç°åº¦å›¾åƒ
            arr = arr[:,np.newaxis, ...] # B, 1, H, W
            
        elif arr.shape[-1] == 3 or arr.shape[-1] == 4:
            # case2: ä¹Ÿæœ‰å¯èƒ½æ˜¯ H, W, 3/4 éœ€è¦ä¿è¯ä¸º3  -> [H, W, 3]
            arr = np.array(Image.fromarray(np.uint8(arr)).convert("RGB")).astype(np.float32)
            # transpose h,w,3 -> 3,h,w
            arr = np.transpose(arr, (2, 0, 1))
            # 3,h,w -> 1, 3, h, w   1 is batch
            arr = arr[np.newaxis, ...]
        else:
            print("Please convert image to gray or RGB first!")
            
    elif arr.ndim == 4:
        # case1 è¾“å…¥ä¸ºB, H, W, C -> B, C, H, W
        arr = np.transpose(arr, (0, 3, 1, 2))
    
    
    # ç»Ÿä¸€åˆ¤æ–­  æ­¤æ—¶ï¼Œarr çš„ shape ä¸º N x C_in x H x W
    # B, C, H, W æŠŠCç»Ÿä¸€è½¬æˆ3
    C_in = arr.shape[1]
    
    # 3. ç»Ÿä¸€é€šé“åŒ¹é… (ç›®æ ‡ C = 3)
    if C_in != TARGET_CHANNELS:
        if C_in == 1:
            # ç°åº¦ (N x 1 x H x W) -> RGB (N x 3 x H x W)
            arr = np.repeat(arr, TARGET_CHANNELS, axis=1)
        elif C_in == 4:
            # RGBA (N x 4 x H x W) -> RGB (N x 3 x H x W)
            arr = arr[:, :TARGET_CHANNELS, :, :]
        elif C_in == TARGET_CHANNELS:
            # å·²ç»æ˜¯ 3 é€šé“
            pass
        else:
            raise ValueError(f"Cannot match input channels {C_in} to target {TARGET_CHANNELS}")
    
    
    # 4. åº”ç”¨ mmseg é£æ ¼å½’ä¸€åŒ– (ä½¿ç”¨ NumPy å¹¿æ’­æé«˜æ•ˆç‡)
    if normalize:
        # é‡å¡‘ mean/std ä¸º 1 x C x 1 x 1ï¼Œä»¥ä¾¿è·¨ N, H, W ç»´åº¦å¹¿æ’­
        mean_b = np.array(mean, dtype=np.float32).reshape(1, TARGET_CHANNELS, 1, 1)
        std_b = np.array(std, dtype=np.float32).reshape(1, TARGET_CHANNELS, 1, 1)
        
        arr = (arr - mean_b) / std_b
        
    # 5. æœ€ç»ˆè¾“å‡º: N x 3 x H x W Tensor
    return torch.from_numpy(arr)




@timer
def infer_numpy(model, image: np.ndarray, device=None):
    """
    Run inference on single image (numpy array).
    Run inference on batch image (numpy array).
    
    Returns mask as uint8.
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        

    model.eval()
    model.to(device)
    
    x = prepare_image(image, in_channels=3).to(device)
    
    with torch.no_grad():
        out, _ = model(x)   # 1,C,H,W
        # print(out.shape)
        mask = torch.argmax(out, dim=1)
        mask = mask.cpu().numpy().squeeze()

    return mask.astype(np.uint8)


# ============================================
# 1. æ•´å›¾æ¨ç†ï¼ˆæ”¯æŒ resizeï¼‰ æ”¯æŒstackã€batch image æ”¯æŒsingle image
# ============================================
@timer
def infer_full_image(model,
                     image: np.ndarray,
                     input_size=None,  # å¦‚ (512,512)ï¼ŒNone åˆ™ä¸ resize
                     device=None):
    """
    æ•´å›¾æ¨ç†
    - image: HWC, HW, CHW
    - input_size: (H, W) æˆ– None
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ä¿å­˜åŸå°ºå¯¸
    # H, W = image.shape[:2]
    image = prepare_image(image, in_channels=3).to(device)
    B,C,H,W = image.shape

    # æ˜¯å¦ resize è¾“å…¥å›¾åƒ
    if input_size is not None:
        image = F.interpolate(image, size=input_size, mode='bilinear', align_corners=False)

    with torch.no_grad():
        preds = []
        for i in range(B):
            out, _ = model(image[i:i+1,:,:,:])  # B,C,h,w
            if input_size is not None:
                out = F.interpolate(out, size=(H, W), mode='bilinear', align_corners=False)
            pred = torch.argmax(out, dim=1)
            preds.append(pred.cpu().numpy().squeeze())
        preds = np.stack(preds, axis=0)
    
    return preds



# ä¸ºäº†è·å–æ¨¡å‹çš„è¾“å‡ºé€šé“æ•°ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°è¿›è¡Œå•æ¬¡åˆ‡ç‰‡æ¨ç†
def _infer_tile_logits(model, tile_img, input_size = None, device=None) -> np.ndarray:
    """
    å¯¹å•ä¸ªåˆ‡ç‰‡æ‰§è¡Œæ¨¡å‹æ¨ç†ï¼Œå¹¶è¿”å›åŸå§‹ logits (CxH_outxW_out, NumPy æ•°ç»„)ã€‚
    tile_img : torch.tensor: [b, c, h, w]
    
    """
    
    # 1. å¯é€‰ï¼šResize (å¦‚æœæœ‰ input_size ä¸”å®ƒä¸ç­‰äºå½“å‰çš„åˆ‡ç‰‡å°ºå¯¸)
    _, _, h, w = tile_img.shape
    x_in = tile_img
    
    if input_size is not None and (h != input_size[0] or w != input_size[1]):
        x_in = F.interpolate(tile_img, size=input_size, mode='bilinear', align_corners=False)
        
        # tile_to_model = resize(tile_img, input_size, preserve_range=True).astype(tile_img.dtype)
    
    # 2. prepare_image: (H,W,C) -> (1, C, H, W) Tensor
    # x_in: torch.Tensor = prepare_image(tile_to_model, in_channels=3).to(device)
    
    
    # 3. æ¨¡å‹æ¨ç†
    with torch.no_grad():
        # å‡è®¾æ‚¨çš„æ¨¡å‹è¿”å› (logits, aux_output)
        logits, _ = model(x_in) # 1xNum_ClassesxH_outxW_out
        
        # å°† Logits resize å›åˆ‡ç‰‡åŸå§‹å¤§å° (Logitsæ’å€¼å¿…é¡»ç”¨ torch)
        # å¦‚æœæ¨¡å‹è¾“å‡ºå°ºå¯¸ä¸åˆ‡ç‰‡åŸå§‹å°ºå¯¸ä¸åŒï¼Œéœ€è¦æ’å€¼å›åˆ‡ç‰‡å°ºå¯¸
        if logits.shape[2] != h or logits.shape[3] != w:
             # resize logits (1, C, H_out, W_out) -> (1, C, H_tile, W_tile)
             logits = F.interpolate(
                logits, 
                size=(h, w), 
                mode='bilinear', 
                align_corners=False # åˆ†å‰²ä»»åŠ¡ä¸­å¸¸ç”¨
             )
        
        return logits.squeeze(0).cpu().numpy() # CxH_tilexW_tile NumPy

@timer
def infer_sliding_window(
    model: torch.nn.Module, 
    image: np.ndarray, 
    window_size: int, # å¯¹åº” crop_size çš„ H å’Œ W (å‡è®¾ä¸ºæ–¹å½¢)
    overlap: float = 0.25,
    out_channels: int=2,
    img_size = None, # å¯¹åº”æ¨¡å‹è¾“å…¥å°ºå¯¸ (H_in, W_in)
    device = None,
) -> np.ndarray:
    """
    MMseg é£æ ¼çš„æ»‘åŠ¨çª—å£æ¨ç† (Logits ç´¯åŠ å¹³å‡)ã€‚
    
    - image: HxWxC æˆ– HxW çš„è¾“å…¥å›¾åƒ (NumPy æ•°ç»„)ã€‚
    - window_size: æ»‘çª—çš„è¾¹é•¿ (H_crop = W_crop)ã€‚
    - overlap: é‡å æ¯”ä¾‹ (0.0 åˆ° 1.0)ã€‚
    - img_size: æ¨¡å‹è¾“å…¥å¤§å° (H_in, W_in) æˆ– Noneã€‚
        - å¦‚æœ img_size != None, æ¯ä¸ªåˆ‡ç‰‡ä¼šè¢« resize åˆ° img_size åè¾“å…¥æ¨¡å‹ï¼Œ
          æ¨ç†ç»“æœ logits å†è¢« resize å›åˆ‡ç‰‡åŸå§‹å¤§å°ã€‚
    
    Returns:
        np.ndarray: æœ€ç»ˆçš„åˆ†å‰²ç»“æœ mask (H x W, np.uint8)ã€‚
    """

    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    model.eval()
    model.to(device)

   
    image = prepare_image(image, in_channels=3).to(device) #  return N x 3 x H x W Tensor
    batch, C, H_img, W_img = image.shape
    # H_img, W_img = image.shape[-2], image.shape[-1]
    H_crop, W_crop = window_size, window_size # å‡è®¾çª—å£ä¸ºæ–¹å½¢
 
    # 2. å‡†å¤‡ Logits ç´¯åŠ å›¾å’Œè®¡æ•°å›¾
    preds = np.zeros((out_channels, H_img, W_img), dtype=np.float32) # CxHxW Logits
    count_mat = np.zeros((H_img, W_img), dtype=np.float32)           # HxW è®¡æ•°
    
    # 3. è®¡ç®—æ­¥é•¿å’Œç½‘æ ¼
    # stride = window_size * (1 - overlap)
    h_stride = int(H_crop * (1.0 - overlap))
    w_stride = int(W_crop * (1.0 - overlap))
    h_stride = max(1, h_stride)
    w_stride = max(1, w_stride)

    # è®¡ç®—ç½‘æ ¼æ•°é‡
    h_grids = max(H_img - H_crop + h_stride - 1, 0) // h_stride + 1
    w_grids = max(W_img - W_crop + w_stride - 1, 0) // w_stride + 1
    
    final_masks = []
    for i in range(batch):
        # 4. æ»‘åŠ¨çª—å£å¾ªç¯
        for h_idx in range(h_grids):
            for w_idx in range(w_grids):
                y1_orig = h_idx * h_stride
                x1_orig = w_idx * w_stride
                
                # MMseg é£æ ¼çš„è¾¹ç•Œå¤„ç†ï¼šç¡®ä¿åˆ‡ç‰‡ä¸è¶…è¿‡å›¾åƒè¾¹ç•Œï¼Œä¸”è´´åˆè¾¹ç•Œ
                y2 = min(y1_orig + H_crop, H_img)
                x2 = min(x1_orig + W_crop, W_img)
                
                # ä¿®æ­£èµ·å§‹åæ ‡ï¼Œä»¥ç¡®ä¿è¾¹ç¼˜åˆ‡ç‰‡ä¹Ÿå°½é‡ä½¿ç”¨å®Œæ•´ crop_size
                y1 = max(y2 - H_crop, 0)
                x1 = max(x2 - W_crop, 0)
                
                # åˆ‡ç‰‡ (H_tile, W_tile, C) æˆ– (H_tile, W_tile)
                # tile -> tensor: [b,C, H_tile, W_tile]
                tile = image[i:i+1, :, y1:y2, x1:x2] 
                
                # 5. æ¨ç†å¹¶è·å– Logits
                # crop_seg_logit_resized: CxH_tilexW_tile NumPy Logits
                crop_seg_logit_resized = _infer_tile_logits(model, tile, img_size, device)
                
                # 6. ç´¯åŠ  Logits å’Œ è®¡æ•°
                # Logits ç´¯åŠ  (CxHxW)
                preds[:, y1:y2, x1:x2] += crop_seg_logit_resized
                
                # è®¡æ•°ç´¯åŠ  (HxW)
                count_mat[y1:y2, x1:x2] += 1

        # 7. è®¡ç®—å¹³å‡ Logits
        count_mat[count_mat == 0] = 1 # é¿å…é™¤ä»¥é›¶
        
        # æ‰©å±• count_mat ç»´åº¦ä»¥è¿›è¡Œé€é€šé“å¹³å‡
        count_mat_expanded = np.expand_dims(count_mat, axis=0) # 1xHxW
        avg_seg_logits = preds / count_mat_expanded             # CxHxW

        # 8. æœ€ç»ˆ Argmax
        # Argmax on channel dimension to get final prediction mask (HxW)
        final_mask = np.argmax(avg_seg_logits, axis=0).astype(np.uint8)
        final_masks.append(final_mask)
        
    final_masks = np.stack(final_masks, axis=0)
    
    return final_masks



    
    
